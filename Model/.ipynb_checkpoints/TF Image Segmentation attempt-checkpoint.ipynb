{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories declaration and initiating the patient names\n",
    "\n",
    "sampled_pickles = '/home/omar/Desktop/sampled_output/'\n",
    "\n",
    "\n",
    "patients = os.listdir(sampled_pickles)\n",
    "random.shuffle(patients)\n",
    "# training_patients = patients[:26]\n",
    "training_patients = ['TCGA-76-6282.pickle', 'TCGA-76-4932.pickle', 'TCGA-02-0046.pickle', 'TCGA-06-0138.pickle', 'TCGA-76-6657.pickle', 'TCGA-76-4935.pickle', 'TCGA-19-5960.pickle', 'TCGA-76-6280.pickle', 'TCGA-02-0006.pickle', 'TCGA-19-1789.pickle', 'TCGA-02-0009.pickle', 'TCGA-19-0963.pickle', 'TCGA-02-0086.pickle', 'TCGA-06-0149.pickle', 'TCGA-06-0179.pickle', 'TCGA-06-2570.pickle', 'TCGA-02-0075.pickle', 'TCGA-12-0616.pickle', 'TCGA-02-0085.pickle', 'TCGA-06-5417.pickle', 'TCGA-06-0145.pickle', 'TCGA-08-0355.pickle', 'TCGA-02-0054.pickle', 'TCGA-19-2624.pickle', 'TCGA-06-0177.pickle', 'TCGA-02-0106.pickle']\n",
    "# test_patients = patients[26:29]\n",
    "test_patients = ['TCGA-08-0509.pickle', 'TCGA-08-0520.pickle', 'TCGA-06-6389.pickle']\n",
    "image_width = 256\n",
    "image_height = 256\n",
    "# batch_size= 16\n",
    "current_dir = '/home/omar/Desktop/Generalization/Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data npy files and loading model-suitable images\n",
    "def view_original(set_of_patients, npy_train, npy_mask):\n",
    "    for patient in set_of_patients:\n",
    "        path = sampled_pickles + patient\n",
    "        pickle_file = open(\"%s\"%(path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        npy_array=0\n",
    "        \n",
    "        for scan in range(len(patient_data)):\n",
    "            if scan!=4:\n",
    "                for array in range(5):\n",
    "                    plt.figure(figsize=(15,15))\n",
    "                    plt.subplot(221)\n",
    "                    plt.imshow(np.squeeze(patient_data[scan][array]), 'gray')\n",
    "                    plt.subplot(222)\n",
    "                    plt.imshow(np.squeeze(patient_data[4][scan][array]), 'gray')\n",
    "                    plt.subplot(223)\n",
    "                    plt.imshow(npy_train[npy_array], 'gray')\n",
    "                    plt.subplot(224)\n",
    "                    plt.imshow(npy_mask[npy_array], 'gray')\n",
    "                    npy_array+=1\n",
    "        \n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  input_mask -= 1\n",
    "  return input_image, input_mask\n",
    "\n",
    "\n",
    "def npy_generator(training, testing):\n",
    "    path = sampled_pickles\n",
    "    out_dir = '/home/omar/Desktop/Generalization/Model/TF_SEG_TEST/'\n",
    "    os.chdir(out_dir)\n",
    "    img_height = 256\n",
    "    img_width = 256\n",
    "    total_scans = 0\n",
    "    total_scans_test = 0\n",
    "    \n",
    "    for patient in training:\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        total_scans += len(patient_data[0]) + len(patient_data[1]) + len(patient_data[2]) + len(patient_data[3])\n",
    "    \n",
    "    \n",
    "    for patient in testing:\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        total_scans_test += len(patient_data[0]) + len(patient_data[1]) + len(patient_data[2]) + len(patient_data[3])\n",
    "        \n",
    "    \n",
    "    imgs_train = np.ndarray((total_scans, img_width, img_height), dtype = np.float32)\n",
    "    masks_train = np.ndarray((total_scans, img_width, img_height), dtype = np.float32)\n",
    "    \n",
    "    imgs_test = np.ndarray((total_scans_test, img_width, img_height), dtype = np.float32)\n",
    "    ground_truth = np.ndarray((total_scans, img_width, img_height), dtype = np.float32)\n",
    "    \n",
    "    indexer = 0\n",
    "    for patient in training:\n",
    "        print('Current Patient: %s' % (patient))\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        for scan in range(len(patient_data)):\n",
    "            if scan != 4:\n",
    "                for array in range(len(patient_data[scan])):\n",
    "                    img = patient_data[scan][array]\n",
    "                    img_mask = patient_data[4][scan][array]\n",
    "                    imgs_train[indexer] = img\n",
    "                    masks_train[indexer] = img_mask\n",
    "                    indexer+=1\n",
    "    np.save('imgs_train.npy', imgs_train)\n",
    "    np.save('img_masks_train.npy', masks_train)\n",
    "    \n",
    "    indexer_test = 0\n",
    "    for patient in testing:\n",
    "        print('Current Patient: %s' % (patient))\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        for scan in range(len(patient_data)):\n",
    "            if scan != 4:\n",
    "                for array in range(len(patient_data[scan])):\n",
    "                    img = patient_data[scan][array]\n",
    "                    img_mask = patient_data[4][scan][array]\n",
    "                    imgs_test[indexer_test] = img\n",
    "                    ground_truth[indexer_test] = img_mask\n",
    "                    indexer_test+=1\n",
    "                    \n",
    "    np.save('imgs_test.npy', imgs_test)\n",
    "    np.save('ground_truth.npy', ground_truth)\n",
    "    \n",
    "    print('Training and testing npy files generated.')\n",
    "\n",
    "    \n",
    "\n",
    "def load_image_train():\n",
    "    path = '/home/omar/Desktop/Generalization/Model/TF_SEG_TEST/'\n",
    "    training_dataset = path + 'imgs_train.npy'\n",
    "    mask_dataset = path + 'img_masks_train.npy'\n",
    "    imgs = np.load(training_dataset)\n",
    "    masks = np.load(mask_dataset)\n",
    "#     for arr in range(len(imgs)):\n",
    "#         print(np.shape(imgs[arr]))\n",
    "# #         imgs[arr] = cv2.resize(imgs[arr], (128,128))\n",
    "#         masks[arr] = cv2.resize(masks[arr], (128,128))\n",
    "    print('Loaded training dataset')\n",
    "    return imgs, masks\n",
    "\n",
    "\n",
    "def load_image_test():\n",
    "    path = '/home/omar/Desktop/Generalization/Model/TF_SEG_TEST/'\n",
    "    test_dataset = path + 'imgs_test.npy'\n",
    "    ground_truth = path + 'ground_truth.npy'\n",
    "    imgs = np.load(test_dataset)\n",
    "    masks = np.load(ground_truth)\n",
    "#     for arr in range(len(imgs)):\n",
    "#         imgs[arr] = np.resize(imgs[arr], (128,128))\n",
    "#         masks[arr] = np.resize(masks[arr], (128,128))\n",
    "    print('Loaded testing dataset')\n",
    "    return imgs, masks\n",
    "\n",
    "def batch_norm(npy_train_imgs, npy_train_masks, npy_test_imgs, npy_test_masks):\n",
    "    final_training_images = np.ndarray((npy_train_imgs.shape[0], 256, 256), dtype=np.float32)\n",
    "    final_training_masks = np.ndarray((npy_train_masks.shape[0], 256, 256), dtype=np.float32)\n",
    "    final_testing_images = np.ndarray((npy_test_imgs.shape[0], 256, 256), dtype=np.float32)\n",
    "    final_testing_masks = np.ndarray((npy_test_masks.shape[0], 256, 256), dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    for image in range(len(npy_train_imgs)):\n",
    "        final_training_images[image] = tf.cast(npy_train_imgs[image], tf.float32) / 255.0\n",
    "        final_training_masks[image] = npy_train_masks[image]-1\n",
    "    \n",
    "    for image in range(len(npy_test_imgs)):\n",
    "        final_testing_images[image] = tf.cast(npy_test_imgs[image], tf.float32) / 255.0\n",
    "        final_testing_masks[image] = npy_test_masks[image]-1\n",
    "#     for image in range(len(input_images)):\n",
    "#         train_image = tf.cast(input_images[image], tf.float32) / 255.0\n",
    "#         train_mask = input_masks[image] - 1\n",
    "#         input_images[image] = train_image\n",
    "#         input_masks[image] = train_mask\n",
    "    return final_training_images, final_training_masks, final_testing_images, final_testing_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "steps_per_epoch = 26 // batch_size\n",
    "output_channels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Generating npy files for training and testing datasets\n",
      "Current Patient: TCGA-76-6282.pickle\n",
      "Current Patient: TCGA-76-4932.pickle\n",
      "Current Patient: TCGA-02-0046.pickle\n",
      "Current Patient: TCGA-06-0138.pickle\n",
      "Current Patient: TCGA-76-6657.pickle\n",
      "Current Patient: TCGA-76-4935.pickle\n",
      "Current Patient: TCGA-19-5960.pickle\n",
      "Current Patient: TCGA-76-6280.pickle\n",
      "Current Patient: TCGA-02-0006.pickle\n",
      "Current Patient: TCGA-19-1789.pickle\n",
      "Current Patient: TCGA-02-0009.pickle\n",
      "Current Patient: TCGA-19-0963.pickle\n",
      "Current Patient: TCGA-02-0086.pickle\n",
      "Current Patient: TCGA-06-0149.pickle\n",
      "Current Patient: TCGA-06-0179.pickle\n",
      "Current Patient: TCGA-06-2570.pickle\n",
      "Current Patient: TCGA-02-0075.pickle\n",
      "Current Patient: TCGA-12-0616.pickle\n",
      "Current Patient: TCGA-02-0085.pickle\n",
      "Current Patient: TCGA-06-5417.pickle\n",
      "Current Patient: TCGA-06-0145.pickle\n",
      "Current Patient: TCGA-08-0355.pickle\n",
      "Current Patient: TCGA-02-0054.pickle\n",
      "Current Patient: TCGA-19-2624.pickle\n",
      "Current Patient: TCGA-06-0177.pickle\n",
      "Current Patient: TCGA-02-0106.pickle\n",
      "Current Patient: TCGA-08-0509.pickle\n",
      "Current Patient: TCGA-08-0520.pickle\n",
      "Current Patient: TCGA-06-6389.pickle\n",
      "Training and testing npy files generated.\n",
      "------------------------------\n",
      "Loading training and testing datasets\n",
      "Loaded training dataset\n",
      "Loaded testing dataset\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Running Helper Functions for data generation\n",
    "print('-'*30)\n",
    "print('Generating npy files for training and testing datasets')\n",
    "npy_generator(training_patients, test_patients)\n",
    "print('-'*30)\n",
    "print('Loading training and testing datasets')\n",
    "train_imgs, train_masks = load_image_train()\n",
    "test_imgs, ground_truth = load_image_test()\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_original(training_patients,train_imgs, train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the dataset\n",
    "final_train_norm = tf.cast(train_imgs, tf.float32) / 255.0\n",
    "final_test_norm = tf.cast(test_imgs, tf.float32) / 255.0\n",
    "final_test_imgs = test_imgs-1\n",
    "final_ground_truth = ground_truth - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK IN PROGRESS... MATCH THIS MODEL WITH THE PREPARED DATSET\n",
    "# def UNet(pretrained_weights=None, input_size = (256,256,1)):\n",
    "#     inputs = Input(input_size)\n",
    "#     conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "#     conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "#     conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "#     conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "#     conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "#     conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "#     conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "#     conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "#     drop4 = Dropout(0.5)(conv4)\n",
    "#     pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "#     conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "#     conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "#     drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "#     up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "#     merge6 = concatenate([drop4,up6], axis = 3)\n",
    "#     conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "#     conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "#     up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "#     merge7 = concatenate([conv3,up7], axis = 3)\n",
    "#     conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "#     conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "#     up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "#     merge8 = concatenate([conv2,up8], axis = 3)\n",
    "#     conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "#     conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "#     up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "#     merge9 = concatenate([conv1,up9], axis = 3)\n",
    "#     conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "#     conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "#     conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "#     conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "#     model = Model(input = inputs, output = conv10)\n",
    "\n",
    "#     model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
