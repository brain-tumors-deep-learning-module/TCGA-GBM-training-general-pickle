{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the section dedicated to contatain the results of the sampling\n",
    "# Purpose here is to try and use the UNET architecture and understand the\n",
    "# first steps of building and training the neural network\n",
    "sampled_pickles = '/home/omar/Desktop/sampled_output/'\n",
    "smooth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to choose random 26 patients as training pool. The\n",
    "# remaining 3 are our test pool.\n",
    "\n",
    "patients = os.listdir(sampled_pickles)\n",
    "random.shuffle(patients)\n",
    "training_patients = patients[:26]\n",
    "test_patients = patients[26:29]\n",
    "image_width = 256\n",
    "image_height = 256\n",
    "# batch_size= 16\n",
    "current_dir = '/home/omar/Desktop/Generalization/Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path, list_of_patients):\n",
    "    # Setting the lists that will contain the images and masks\n",
    "    img_height = 256\n",
    "    img_width = 256\n",
    "    total_scans = 0\n",
    "    seg = 0\n",
    "    for patient in list_of_patients:\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        total_scans += len(patient_data[0]) + len(patient_data[1]) + len(patient_data[2]) + len(patient_data[3])\n",
    "    \n",
    "    imgs = np.ndarray((total_scans, img_width, img_height), dtype = np.uint8)\n",
    "    masks = np.ndarray((total_scans, img_width, img_height), dtype = np.uint8)\n",
    "    \n",
    "    indexer = 0\n",
    "    for patient in list_of_patients:\n",
    "        print('Current Patient: %s' % (patient))\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        for scan in range(len(patient_data)):\n",
    "            if scan != 4:\n",
    "                for array in range(len(patient_data[scan])):\n",
    "                    img = patient_data[scan][array]\n",
    "                    img_mask = patient_data[4][scan][array]\n",
    "                    imgs[indexer] = img\n",
    "                    masks[indexer] = img_mask\n",
    "                    indexer+=1\n",
    "    os.chdir(current_dir)\n",
    "    np.save('imgs_train.npy', imgs)\n",
    "    np.save('img_masks_train.npy', masks)\n",
    "    print('Images for training and masking saved to .npy files.')\n",
    "\n",
    "    \n",
    "def load_data():\n",
    "    imgs = np.load('imgs_train.npy')\n",
    "    masks = np.load('img_masks_train.npy')\n",
    "    return imgs, masks\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(imgs):\n",
    "    processed_images =np.ndarray((imgs.shape[0], image_width, image_height), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        processed_images[i] = resize(imgs[i], (image_width, image_height), preserve_range=True)\n",
    "    \n",
    "    processed_images = processed_images[..., np.newaxis]\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "\n",
    "def prepare_test(path, list_of_patients):\n",
    "    img_height = 256\n",
    "    img_width = 256\n",
    "    total_scans = 0\n",
    "    seg = 0\n",
    "    for patient in list_of_patients:\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        total_scans += len(patient_data[0]) + len(patient_data[1]) + len(patient_data[2]) + len(patient_data[3])\n",
    "    \n",
    "    imgs = np.ndarray((total_scans, image_width, image_height), dtype = np.uint8)\n",
    "    ids = np.ndarray((total_scans, ), dtype=np.int32)\n",
    "    \n",
    "    indexer = 0\n",
    "    for patient in list_of_patients:\n",
    "        print('Current Patient: %s' % (patient))\n",
    "        patient_path = path + patient\n",
    "        pickle_file = open(\"%s\"%(patient_path), \"rb\")\n",
    "        patient_data = pickle.load(pickle_file)\n",
    "        for scan in range(len(patient_data)):\n",
    "            if scan != 4:\n",
    "                for array in range(len(patient_data[scan])):\n",
    "                    img = patient_data[scan][array]\n",
    "                    imgID = indexer+1\n",
    "                    imgs[indexer] = img\n",
    "                    ids[indexer] = imgID\n",
    "                    indexer+=1\n",
    "    os.chdir(current_dir)\n",
    "    np.save('imgs_test.npy', imgs)\n",
    "    np.save('img_id_test.npy', masks)\n",
    "    print('Images for testing and ids saved to .npy files.')\n",
    "    \n",
    "\n",
    "def load_test():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_id = np.load('img_id_test.npy')\n",
    "    return imgs_test, imgs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def UNet():\n",
    "    inputs = Input((image_width, image_height, 1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset\n",
      "['TCGA-76-6282.pickle', 'TCGA-76-4932.pickle', 'TCGA-02-0046.pickle', 'TCGA-06-0138.pickle', 'TCGA-76-6657.pickle', 'TCGA-76-4935.pickle', 'TCGA-19-5960.pickle', 'TCGA-76-6280.pickle', 'TCGA-02-0006.pickle', 'TCGA-19-1789.pickle', 'TCGA-02-0009.pickle', 'TCGA-19-0963.pickle', 'TCGA-02-0086.pickle', 'TCGA-06-0149.pickle', 'TCGA-06-0179.pickle', 'TCGA-06-2570.pickle', 'TCGA-02-0075.pickle', 'TCGA-12-0616.pickle', 'TCGA-02-0085.pickle', 'TCGA-06-5417.pickle', 'TCGA-06-0145.pickle', 'TCGA-08-0355.pickle', 'TCGA-02-0054.pickle', 'TCGA-19-2624.pickle', 'TCGA-06-0177.pickle', 'TCGA-02-0106.pickle']\n",
      "Test Dataset\n",
      "['TCGA-08-0509.pickle', 'TCGA-08-0520.pickle', 'TCGA-06-6389.pickle']\n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset')\n",
    "print(training_patients)\n",
    "print(\"Test Dataset\")\n",
    "print(test_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Generating Data and masks\n",
      "------------------------------\n",
      "Current Patient: TCGA-76-6282.pickle\n",
      "Current Patient: TCGA-76-4932.pickle\n",
      "Current Patient: TCGA-02-0046.pickle\n",
      "Current Patient: TCGA-06-0138.pickle\n",
      "Current Patient: TCGA-76-6657.pickle\n",
      "Current Patient: TCGA-76-4935.pickle\n",
      "Current Patient: TCGA-19-5960.pickle\n",
      "Current Patient: TCGA-76-6280.pickle\n",
      "Current Patient: TCGA-02-0006.pickle\n",
      "Current Patient: TCGA-19-1789.pickle\n",
      "Current Patient: TCGA-02-0009.pickle\n",
      "Current Patient: TCGA-19-0963.pickle\n",
      "Current Patient: TCGA-02-0086.pickle\n",
      "Current Patient: TCGA-06-0149.pickle\n",
      "Current Patient: TCGA-06-0179.pickle\n",
      "Current Patient: TCGA-06-2570.pickle\n",
      "Current Patient: TCGA-02-0075.pickle\n",
      "Current Patient: TCGA-12-0616.pickle\n",
      "Current Patient: TCGA-02-0085.pickle\n",
      "Current Patient: TCGA-06-5417.pickle\n",
      "Current Patient: TCGA-06-0145.pickle\n",
      "Current Patient: TCGA-08-0355.pickle\n",
      "Current Patient: TCGA-02-0054.pickle\n",
      "Current Patient: TCGA-19-2624.pickle\n",
      "Current Patient: TCGA-06-0177.pickle\n",
      "Current Patient: TCGA-02-0106.pickle\n",
      "Images for training and masking saved to .npy files.\n",
      "------------------------------\n",
      "Images and mask data created.\n"
     ]
    }
   ],
   "source": [
    "# Creating and saving data selected by randomizer.\n",
    "print('-'*30)\n",
    "print('Generating Data and masks')\n",
    "print('-'*30)\n",
    "prepare_data(sampled_pickles, training_patients)\n",
    "print('-'*30)\n",
    "print('Images and mask data created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading created data.\n",
      "------------------------------\n",
      "Data Loaded Successfully.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loading data from npy files created.\n",
    "print('-'*30)\n",
    "print(\"Loading created data.\")\n",
    "print('-'*30)\n",
    "imgs, masks = load_data()\n",
    "print('Data Loaded Successfully.')\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training with loaded data initiation.\n",
      "------------------------------\n",
      "Preprocessing images and masks\n",
      "------------------------------\n",
      "Compiling model.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Training with loaded data initiation.')\n",
    "print('-'*30)\n",
    "print('Preprocessing images and masks')\n",
    "\n",
    "img_final = preprocess(imgs)\n",
    "mask_final = preprocess(masks)\n",
    "imgs_train = img_final.astype('float32')\n",
    "mean = np.mean(imgs_train)\n",
    "std = np.std(imgs_train)\n",
    "\n",
    "imgs_train -= mean\n",
    "imgs_train /= std\n",
    "\n",
    "masks_train = mask_final.astype('float32')\n",
    "masks_train /= 255\n",
    "\n",
    "print('-'*30)\n",
    "print('Compiling model.')\n",
    "\n",
    "\n",
    "model = UNet()\n",
    "model_checkpoint = ModelCheckpoint('weights.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Complete.\n"
     ]
    }
   ],
   "source": [
    "# print('-'*30)\n",
    "# print('Fitting Data.')\n",
    "# print('-'*30)\n",
    "\n",
    "# model.fit(imgs_train, masks_train, batch_size=32, nb_epoch=20, verbose=1, shuffle=True,\n",
    "#               validation_split=0.2,\n",
    "#               callbacks=[model_checkpoint])\n",
    "print('Fitting Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating Test Data.\n",
      "------------------------------\n",
      "Current Patient: TCGA-08-0509.pickle\n",
      "Current Patient: TCGA-08-0520.pickle\n",
      "Current Patient: TCGA-06-6389.pickle\n",
      "Images for testing and ids saved to .npy files.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generating the test patients\n",
    "print('-'*30)\n",
    "print('Creating Test Data.')\n",
    "print('-'*30)\n",
    "prepare_test(sampled_pickles, test_patients)\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing the test patient data.\n",
    "print('-'*30)\n",
    "print('Loading and preprocessing test data...')\n",
    "print('-'*30)\n",
    "imgs_test, imgs_id_test = load_test()\n",
    "\n",
    "imgs_test = preprocess(imgs_test)\n",
    "\n",
    "imgs_test = imgs_test.astype('float32')\n",
    "imgs_test -= mean\n",
    "imgs_test /= std\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading saved weights...')\n",
    "print('-'*30)\n",
    "model.load_weights('weights.h5')\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "print('Predicting masks on test data...')\n",
    "print('-'*30)\n",
    "imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "out_dir = '/home/omar/Desktop/Generalization/Model/OUT/'\n",
    "os.chdir(out_dir)\n",
    "np.save('imgs_mask_test.npy', imgs_mask_test)\n",
    "print('-' * 30)\n",
    "print('Saving predicted masks to files...')\n",
    "print('-' * 30)\n",
    "pred_dir = 'preds'\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.mkdir(pred_dir)\n",
    "for image, image_id in zip(imgs_mask_test, imgs_id_test):\n",
    "    ind = 1\n",
    "    image = (image[:, :, 0] * 255.).astype(np.uint8)\n",
    "    imsave(os.path.join(pred_dir, str(ind) + '_pred.png'), image)\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
